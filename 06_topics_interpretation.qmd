---
title: "LLM Interpretation of Topic Models"
author: "Doron Feingold"
date: "today"
format: 
  html:
    toc: true
    toc-location: left
    self-contained: true
    theme: cosmo
    code-overflow: wrap
---

## Overview
This document uses a Large Language Model (LLM) to interpret the topics generated by our LDA models in the previous step. The 'topics' generated by the models are collections of statistically related words. While these may be meaningful to a subject-matter expert, manual interpretation is a laborious process that can lead to inevitable subjectivity. We therefore chose to use an LLM to automate this step, prioritizing speed and reproducibility.

For this task, we selected Anthropic's Claude model, which has a reputation for nuanced cultural and historical interpretation. We'll use its API to interpret the topics from each of our k models, providing it with the highest-probability words (ranked by their beta values) for each topic.

## 1. Setup

First, we load the necessary libraries and data.

```{r setup, message=FALSE, warning=FALSE, echo = TRUE}
# Load libraries
library(dplyr)
library(tidytext)
library(tidyr)
library(stringr)
library(ggplot2)
library(topicmodels)
library(purrr)
library(httr2)
library(jsonlite)
library(readr)
library(kableExtra)

# Load project-specific functions
source("R/functions.R")

# Load trained models and corpus metadata
lda_models <- readRDS("output/models/lda_models.rds")
k_values <- c(4, 6, 8, 10, 12)

# Set up API Key
ANTHROPIC_API_KEY <- Sys.getenv("ANTHROPIC_API_KEY")
```

## 2. Prompt Engineering for Interpretation
We create a new function `interpret_topic_set_with_llm` that formats the top words from all topics in a model into a single prompt. The prompt instructs the LLM to return a JSON array, with one object for each topic's interpretation.

### Prompt:

```{txt}
You are an expert political historian specializing in Canadian policy. 
I have a set of {k_value} topics from a topic model of Canadian Throne Speeches. 
Your task is to provide a concise interpretation for EACH topic based on its most probable words, considering the context of the other topics.

Here is the full set of topics and their words: {topics_words}

Please provide your response as a valid JSON array (and ONLY the JSON array, no markdown formatting or extra text). 

Each element should be an object with these three keys:
1. 'topic_id': The integer topic number.
2. 'label': A short, descriptive topic label of 3-5 words (e.g., 'National Defense & Foreign Affairs').
3. 'focus': A single sentence describing the policy area this topic represents.

Return only the JSON array, starting with [ and ending with ]:
```


## 3. Iteratively Interpret Each Topic Set
Now we'll loop through each model, making a call using `interpret_topic_set_with_llm`, and parse the JSON array result.

**Note**: To reproduce this part you must have an API key with Anthropic. It is not free but $5 credit (the minimum amount) should be enough. Running this loop several times during testing, cost about $0.15. It should not be hard to convert the function to another LLM if cost is an issue.

```{r iterative_llm_interpretation, eval = FALSE, echo = TRUE}
# This code iterates through each MODEL
llm_interpretations <- purrr::map2_dfr(
  lda_models,
  k_values,
  function(model, k) {
    # Get the top 15 terms for all topics in THIS model
    topics_for_model <- tidy(model, matrix = "beta") %>%
      group_by(topic) %>%
      slice_max(beta, n = 25) %>%
      summarise(terms = paste(term, collapse = ", "), .groups = "drop")

    result <- interpret_topic_set_with_llm(
      k,
      topics_for_model,
      ANTHROPIC_API_KEY
    )

    if (!is.null(result$error)) {
      log_message(
        paste("API ERROR for k=", k, ":", result$error),
        "llm_interpretation_log.txt"
      )
      return(tibble(
        k = k,
        topic = NA,
        label = "API_ERROR",
        focus = result$error
      ))
    }

    json_text <- result$content[[1]]$text

    log_message(
      paste("Raw response for k=", k, ":", substr(json_text, 1, 200), "..."),
      "llm_interpretation_log.txt"
    )

    clean_json <- clean_json_response(json_text)

    parsed_json <- tryCatch(
      {
        fromJSON(clean_json, flatten = TRUE)
      },
      error = function(e) {
        log_message(
          paste(
            "JSON parsing error for k=",
            k,
            ":",
            e$message,
            "\nCleaned JSON:",
            clean_json
          ),
          "llm_interpretation_log.txt"
        )
        return(NULL)
      }
    )

    if (is.null(parsed_json)) {
      return(tibble(
        k = k,
        topic = NA,
        label = "JSON_PARSE_ERROR",
        focus = "Could not parse JSON response"
      ))
    }

    # Return a tidy data frame, adding the k-value and topic words
    parsed_json %>%
      as_tibble() %>%
      rename(topic = topic_id) %>%
      mutate(k = k) %>%
      left_join(topics_for_model, by = "topic")
  }
)

# Save the resulting interpretations
saveRDS(llm_interpretations, "output/models/llm_topic_labels.rds")
```

## Conclusion 
Now that we have interpretations for our topics, we are ready to analyze the models' gamma valuesâ€”the proportion of each topic within an individual speech. In the final step, we will visualize how these topic proportions change over time, mapping the thematic shifts in Canada's political discourse.

## Appendix: Detailed Topic Interpretations 

```{r display-interpretation-tables, echo=FALSE, results='asis'}
# Load interpretations previously created for illustrating the results
llm_interpretations <- readRDS("output/models/llm_topic_labels.rds")


# Use purrr::walk to create a separate table for each k
purrr::walk(k_values, function(k_val) {
  # Print a markdown header for each table
  cat(paste0("\n\n### Interpretations for k = ", k_val, "\n\n"))

  table_data <- llm_interpretations %>%
    filter(k == k_val) %>%
    # Select and rename columns for a clean display
    select(
      Topic = topic,
      `LLM Label` = label,
      `LLM Focus` = focus,
      `Top 15 Words Sent to LLM` = terms
    )

  kable_table <- knitr::kable(
    table_data,
    caption = paste("LLM Interpretations for", k_val, "Topics")
  ) %>%
    kable_styling(
      bootstrap_options = c("striped", "condensed"),
      full_width = FALSE
    )

  print(kable_table)
})
```


