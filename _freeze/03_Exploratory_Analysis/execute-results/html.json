{
  "hash": "3c4bb9a2c5ae0ab17cfcec25eed663ce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploratory Analysis of Throne Speeches\"\nauthor: \"Doron Feingold\"\ndate: \"August 20, 2025\"\nformat: \n  html:\n    toc: true\n    toc-location: right\n    self-contained: true\n    theme: cosmo\n    code-overflow: wrap\n\n\n---\n\n## Overview\n\nThis document performs an initial exploratory data analysis (EDA) on the segmented corpus of Canadian Throne Speeches. The goal is to uncover broad patterns in the text data over time, focusing on word frequencies, repeated phrases (n-grams), and key terms that characterize different eras.\n\n---\n\n## Setup \n\nFirst, we load the required libraries for data manipulation, text analysis, and visualization. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1867)\n\n# Data Manipulation and Analysis\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(textstem)\nlibrary(tidyr)\n# Visualization\nlibrary(ggplot2)\nlibrary(scales)\n\nsource(\"R/functions.R\")\n```\n:::\n\n\n## Data Loading\nWe then load the segmented corpus and summary files created in the previous step.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the pre-processed data from the segmentation step\nclean_corpus <- readr::read_csv(\n  \"output/segmentation/llm_clean_segmented_corpus.csv\"\n)\nspeech_summaries <- readr::read_csv(\n  \"output/segmentation/llm_speech_summaries.csv\"\n)\n\n# Create a decade field for grouping\nspeeches_df <- clean_corpus %>%\n  mutate(decade = paste0(floor(year / 10) * 10, \"s\"))\n\ncustom_stop_words <- get_project_stopwords()\n\nanalytical_corpus <- speeches_df %>%\n  # Unnest, clean, and lemmatize the words\n  unnest_tokens(word, policy_content) %>%\n  anti_join(custom_stop_words, by = \"word\") %>%\n  mutate(lemma = lemmatize_words(word)) %>%\n  mutate(\n    lemma = case_when(\n      lemma %in% c(\"canadian\", \"canada's\") ~ \"canada\",\n      TRUE ~ lemma\n    )\n  ) %>%\n  # Re-assemble the cleaned text for each speech\n  group_by(filename, date, year, parliament, decade) %>%\n  summarise(\n    clean_policy_text = str_c(lemma, collapse = \" \"),\n    .groups = \"drop\"\n  )\n```\n:::\n\n\n## Basic Text Statistics by Decade\nLet's start by looking at some high-level statistics to see how the speeches have changed over time. We can examine the number of speeches and their average length by decade. Here, we'll focus on the word count of the policy_content section, as this is the core of the speech. \n\n**Note**: The first speech was given in 1867.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 × 3\n   decade number_of_speeches avg_policy_words\n   <chr>               <int>            <dbl>\n 1 1860s                   2             322 \n 2 1870s                  11             808.\n 3 1880s                  10             694.\n 4 1890s                  11             578.\n 5 1900s                  11             711.\n 6 1910s                  11             642.\n 7 1920s                  10             934 \n 8 1930s                  12             875.\n 9 1940s                  13             928.\n10 1950s                  14             953.\n11 1960s                  11            2176.\n12 1970s                   9            2399.\n13 1980s                   6            3171.\n14 1990s                   5            3585.\n15 2000s                   8            3576 \n16 2010s                   5            3612 \n17 2020s                   3            3460.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](output/docs/output/figuresunnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\nThis initial view shows how the substantive policy portion of the speeches has grown considerably over time, peaking in the mid-to-late 20th century.\n\n## Word Frequency Analysis\nNow, let's identify the most common words used in the policy_content of the speeches. This helps us understand the core vocabulary.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 2\n   lemma             n\n   <chr>         <int>\n 1 work            732\n 2 country         622\n 3 good            514\n 4 economic        509\n 5 province        494\n 6 development     488\n 7 people          466\n 8 world           460\n 9 trade           420\n10 house           418\n11 service         399\n12 economy         398\n13 parliament      389\n14 unite           382\n15 legislation     374\n16 public          367\n17 community       353\n18 opportunity     332\n19 support         325\n20 international   322\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](output/docs/output/figuresunnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\nWords like \"government,\" \"canada,\" and \"development\" are, unsurprisingly, common, reflecting the nature of these speeches.\n\n## N-gram Analysis\nWord frequencies are useful, but context is often lost. N-grams are sequences of N words, which help us find commonly repeated phrases. Let's look at bigrams (2-word phrases) to see what concepts often appear together.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 2\n   bigram                    n\n   <chr>                 <int>\n 1 house common            226\n 2 unite state             172\n 3 gentleman house         104\n 4 public service          100\n 5 unite nation             95\n 6 co operation             91\n 7 last session             90\n 8 province territory       79\n 9 provincial government    78\n10 member house             76\n11 long term                75\n12 health care              74\n13 private sector           71\n14 federal provincial       70\n15 work province            68\n16 submit consideration     57\n17 21st century             53\n18 gentleman senate         53\n19 senate gentleman         53\n20 unite kingdom            52\n```\n\n\n:::\n:::\n\n\nPhrases like \"house common\" , \"united states,\" and  \"government work\"   appear frequently, highlighting key entities and concepts in Canadian political discourse.\n\n**Note**: Lemmatization replaces words with their root form (e.g., “working” -> “work,” \"governmental\" -> \"government\"). This is why a phrase like “government work” appears frequently, as it captures many variations of the original phrasing.\n\n**Next Step:** [Boilerplate Identification →](04_Boilerplate_Identification.qmd)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}