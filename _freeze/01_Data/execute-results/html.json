{
  "hash": "dae263635b72687f82e3aa393eab6d5e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Reproducible Corpus Creation: Scraping and Transcribing Throne Speeches\"\nauthor: \"Doron Feingold\"\ndate: \"August 19, 2025\"\nformat: \n  html:\n    toc: true\n    toc-location: right\n    self-contained: true\n    theme: cosmo\n    code-overflow: wrap\n---\n\n## Overview\n\nThis document provides a complete, reproducible pipeline for creating a high-quality text corpus of Canadian Throne Speeches. The process involves scraping an index page from the Parliament of Canada website, programmatically processing downloadable PDFs using the Gemini 1.5 Vision model for high-quality OCR, and providing an interactive workflow to handle speeches that require manual extraction. The final output is a clean `throne_speeches` directory with text file for each speech, and a `speech_index.csv` file.\n\n---\n\n## Setup and Configuration\n\nFirst, we load the required R packages and configure the environment. This includes securely loading the API key and defining all the helper functions we'll need for scraping, transcribing, saving, and indexing the speeches.\n\n### Libraries\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1867)\n\n# Required Libraries\nlibrary(rvest) # For web scraping\nlibrary(dplyr) # For data manipulation\nlibrary(stringr) # For text cleaning\nlibrary(httr2) # For API requests and downloads\nlibrary(pdftools) # To convert PDFs to images\nlibrary(jsonlite) # To handle JSON\nlibrary(base64enc) # To encode images\nlibrary(purrr) # For functional programming\nlibrary(readr) # For writing files\nlibrary(clipr) # For using save from clipboard\n```\n:::\n\n\n### API Key and Output Directory\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# --- API Key Configuration ---\n# The API key is loaded securely from an environment variable.\nGEMINI_API_KEY <- Sys.getenv(\"GEMINI_API_KEY\")\n\n# --- Define Output Directory ---\noutput_dir <- \"output/throne_speeches_txt/\"\nif (!dir.exists(output_dir)) {\n  dir.create(output_dir)\n}\n```\n:::\n\n\n## Functions\nFunctions are all in a central R script to ensure uniformity across the different steps. Here we use the functions:\n\n* `scrape_throne_speech_links`: takes an html file or URL with the speech index and returns a dataframe with: `parliament`, `session`, `date` and `pdf_url`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Web Scraping and OCR Functions\nscrape_throne_speech_links <- function(html_path) {\n  cat(\"Reading and scraping HTML file to find speeches...\\n\")\n  page <- read_html(html_path)\n  rows <- page %>% html_elements(\".dx-datagrid-table tr[role='row']\")\n  current_parliament <- NA\n\n  speeches_df <- map_dfr(rows, function(row) {\n    if (html_attr(row, \"class\") %>% str_detect(\"dx-group-row\")) {\n      parliament_text <- row %>% html_element(\".dx-group-cell\") %>% html_text()\n      current_parliament <<- str_extract(parliament_text, \"\\\\d+\")\n      return(NULL)\n    }\n\n    if (html_attr(row, \"class\") %>% str_detect(\"dx-data-row\")) {\n      cells <- row %>% html_elements(\"td\")\n      session <- cells[[2]] %>% html_text()\n      journal_date_text <- cells[[3]] %>% html_text()\n      pdf_link_node <- cells[[5]] %>% html_element(\"a\")\n\n      if (is.na(pdf_link_node)) {\n        return(NULL)\n      }\n\n      pdf_url <- pdf_link_node %>% html_attr(\"href\")\n      speech_date <- str_extract(journal_date_text, \"\\\\d{4}-\\\\d{2}-\\\\d{2}\")\n\n      if (str_starts(pdf_url, \"/\")) {\n        pdf_url <- paste0(\"[https://lop.parl.ca](https://lop.parl.ca)\", pdf_url)\n      }\n\n      tibble(\n        parliament = as.numeric(current_parliament),\n        session = as.numeric(session),\n        date = speech_date,\n        pdf_url = pdf_url\n      )\n    }\n  })\n\n  cat(\"Found\", nrow(speeches_df), \"speeches with document links.\\n\")\n  return(speeches_df %>% filter(!is.na(date)) %>% arrange(date))\n}\n```\n:::\n\n\n* `ocr_pdf_with_gemini`: The function converts the PDF files into high-resolution images and submits them to Gemini API with the following prompt:\n\n> \"This image is a page from a bilingual historical document, with    English and French text often in separate columns. Please transcribe ONLY THE ENGLISH TEXT from the page. Ignore the French text completely. Also, ignore page numbers, headers, and footers. Preserve the original paragraph breaks from the English text.\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Transcribes text from a PDF file using the Gemini Vision API.\nocr_pdf_with_gemini <- function(pdf_path, api_key) {\n  cat(\"Step 1: Converting PDF to high-resolution images...\\n\")\n  image_files <- tryCatch(\n    {\n      pdftools::pdf_convert(pdf_path, format = 'png', dpi = 300)\n    },\n    error = function(e) {\n      cat(\"ERROR: pdftools failed to convert\", basename(pdf_path), \"\\n\")\n      return(NULL)\n    }\n  )\n\n  if (is.null(image_files)) {\n    return(\"Error: PDF conversion failed.\")\n  }\n\n  on.exit(file.remove(image_files), add = TRUE)\n\n  cat(\n    \"Step 2: Sending\",\n    length(image_files),\n    \"pages to Gemini for transcription...\\n\"\n  )\n\n  text_from_pages <- map_chr(image_files, function(img_file) {\n    encoded_image <- base64enc::base64encode(img_file)\n    url <- \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent\"\n\n    prompt <- \"This image is a page from a bilingual historical document, with English and French text often in separate columns. Please transcribe ONLY THE ENGLISH TEXT from the page. Ignore the French text completely. Also, ignore page numbers, headers, and footers. Preserve the original paragraph breaks from the English text.\"\n\n    req_body <- list(\n      contents = list(\n        list(\n          parts = list(\n            list(text = prompt),\n            list(\n              inline_data = list(\n                mime_type = \"image/png\",\n                data = encoded_image\n              )\n            )\n          )\n        )\n      ),\n      safetySettings = list(\n        list(category = \"HARM_CATEGORY_HARASSMENT\", threshold = \"BLOCK_NONE\"),\n        list(category = \"HARM_CATEGORY_HATE_SPEECH\", threshold = \"BLOCK_NONE\"),\n        list(\n          category = \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          threshold = \"BLOCK_NONE\"\n        ),\n        list(\n          category = \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          threshold = \"BLOCK_NONE\"\n        )\n      )\n    )\n\n    resp <- tryCatch(\n      {\n        request(url) %>%\n          req_url_query(key = api_key) %>%\n          req_body_json(req_body) %>%\n          req_timeout(90) %>%\n          req_perform()\n      },\n      error = function(e) {\n        cat(\n          \"NETWORK ERROR on page:\",\n          basename(img_file),\n          \"-\",\n          e$message,\n          \"\\n\"\n        )\n        return(NULL)\n      }\n    )\n\n    if (is.null(resp)) {\n      return(paste(\"Error: Network failure on page\", basename(img_file)))\n    }\n\n    if (resp_status(resp) == 200) {\n      body <- resp_body_json(resp)\n      if (!is.null(body$candidates) && length(body$candidates) > 0) {\n        text_response <- body$candidates[[1]]$content$parts[[1]]$text\n        if (!is.null(text_response)) {\n          return(text_response)\n        }\n      }\n      reason <- body$promptFeedback$blockReason %||% \"No text in response\"\n      cat(\n        \"WARNING: API returned OK but no content for page:\",\n        basename(img_file),\n        \"Reason:\",\n        reason,\n        \"\\n\"\n      )\n      return(paste(\"Error: No text returned for page\", basename(img_file)))\n    } else {\n      cat(\n        \"API ERROR on page:\",\n        basename(img_file),\n        \"- Status:\",\n        resp_status(resp),\n        \"\\n\"\n      )\n      return(paste(\"Error: API returned status\", resp_status(resp)))\n    }\n  })\n\n  cat(\"Step 3: Assembling and cleaning the final text...\\n\")\n\n  final_text <- paste(text_from_pages, collapse = \"\\n\\n\") %>%\n    str_replace_all(\"Error: [[:print:]]+\", \"\") %>%\n    str_replace_all(\"\\\\s*\\\\r?\\\\n\\\\s*\", \" \") %>%\n    str_replace_all(\"\\\\s+\", \" \") %>%\n    str_trim()\n\n  return(final_text)\n}\n```\n:::\n\n\n* `save_speech`: is used by `ocr_pdf_with_gemini` to save and index the speeches. It takes in the speech text, date and parliament number to be included in a metadata header:\n\n\n> \\# THRONE SPEECH METADATA  \n> \\# Date: 1950-02-16  \n> \\# Parliament: 21  \n> \\# File created: 2025-08-20  \n> \\# =====================================\n>\n>  A.D. 1950 16TH FEBRUARY 3 IN TESTIMONY WHEREOF We have caused these Our Letters to be made Patent and the Great Seal of Canada to be hereunto affixed. WITNESS: Our Right Trusty and Well-beloved Cousin, ...\n\nSee the original [PDF](https://lop.parl.ca/staticfiles/ParlInfo/Documents/ThroneSpeech/En/21-02-e.pdf)\n\n![screenshot of the PDF document.](figures/pdf_screenshot.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"R/functions.R\")\n```\n:::\n\n\n-----\n\n## Automated Corpus Regeneration\n\nThis block runs the automated part of the process. It scrapes the HTML, then loops through all the direct PDF links to download and transcribe them, using the shared `save_speech` function.\n\n**Note:** The following code chunk is set to `eval=FALSE`. To run the automated regeneration, change the chunk option to `{r run-automated-process, eval=TRUE}` and execute it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scrape the HTML file to get the list of speeches\nhtml_file <- \"data/throne_speeches_raw/Speeches from the Throne.html\"\nall_speeches <- scrape_throne_speech_links(html_file)\n\nskipped_speeches <- tibble()\n\n# Loop through and process all direct PDF links\nfor (i in 1:nrow(all_speeches)) {\n  speech_info <- all_speeches[i, ]\n  cat(paste0(\n    \"Evaluating speech \",\n    i,\n    \" of \",\n    nrow(all_speeches),\n    \": \",\n    speech_info$date,\n    \"\\n\"\n  ))\n\n  is_pdf_link <- str_ends(speech_info$pdf_url, \"\\\\.pdf\")\n\n  if (is_pdf_link) {\n    temp_pdf_path <- tempfile(fileext = \".pdf\")\n\n    tryCatch(\n      {\n        cat(\"Link is a PDF. Downloading from:\", speech_info$pdf_url, \"\\n\")\n        request(speech_info$pdf_url) %>%\n          req_perform(path = temp_pdf_path)\n\n        # Run the OCR function\n        final_text <- ocr_pdf_with_gemini(temp_pdf_path, GEMINI_API_KEY)\n\n        if (!is.null(final_text) && nchar(final_text) > 0) {\n          save_speech(\n            speech_text = final_text,\n            date = speech_info$date,\n            parliament = speech_info$parliament,\n            base_dir = output_dir\n          )\n        } else {\n          cat(\n            \"WARNING: OCR failed or returned empty text for\",\n            speech_info$date,\n            \"\\n\"\n          )\n        }\n      },\n      error = function(e) {\n        cat(\n          \"ERROR: Failed to process\",\n          speech_info$date,\n          \". Error:\",\n          e$message,\n          \"\\n\"\n        )\n      }\n    )\n    if (file.exists(temp_pdf_path)) {\n      file.remove(temp_pdf_path)\n    }\n  } else {\n    cat(\n      \"SKIPPING: Link is not a direct PDF. Adding to manual review list.\\n\"\n    )\n    cat(\"URL:\", speech_info$pdf_url, \"\\n\")\n\n    skipped_speeches <- skipped_speeches %>%\n      bind_rows(\n        tibble(\n          parliament = speech_info$parliament,\n          session = speech_info$session,\n          date = speech_info$date,\n          skipped_url = speech_info$pdf_url,\n          reason = \"Not a direct .pdf link\"\n        )\n      )\n  }\n  Sys.sleep(1)\n}\n\n# Save the list of skipped speeches to a CSV file\nif (nrow(skipped_speeches) > 0) {\n  write_csv(skipped_speeches, \"_skipped_for_manual_review.csv\")\n  cat(\n    \"\\nATTENTION: A list of skipped speeches was saved to '_skipped_for_manual_review.csv'\\n\"\n  )\n}\n\n# Create the index for all automatically processed speeches\ncreate_speech_index(base_dir = output_dir)\n\ncat(\"Automated corpus regeneration complete!\\n\")\n```\n:::\n\n\n-----\n\n## Processing Skipped Speeches\n\nSince recent speeches are no longer in the form of scanned PDFs, we skipped them in the previous step and produced a `_skipped_for_manual_review.csv` file.  \n\n### Processing Exceptional Speech \nThe speech from 1989, 3rd session of the 34th Parliament, is a bit tricky. It is in the form of an interactive pdf viewer which lets you view one page at a time. I ended up downloading the complete PDF (\"print\"/save as PDF) from a pdf reader and only \"printing\" the pages with the speech. The pdf is included in the data folder and can be processed by passing it to `ocr_pdf_with_gemini` and `save_speech` for processing uniformity.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npdf <- \"data/throne_speeches_raw/pdf/34_03.pdf\"\n\ntext <- ocr_pdf_with_gemini(pdf, GEMINI_API_KEY)\n\nsave_speech(\n  speech_text = text,\n  date = \"1991-05-13\",\n  parliament = 34\n)\n```\n:::\n\n\n\n### Interactive Processing\nThe code in the next cell will prompt you in the R console to manually copy the text for each of the remaining speeches. A URL for each speech is provided for convenience. Following the URL lets you select and copy the speech to the clipboard (Ctrl+C) and then simply hit Enter in the R console. \n\n**Note:** This chunk must be run **interactively in the R console**. It will not work when rendering the document.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## INTERACTIVE SESSION FOR SKIPPED SPEECHES\nskipped_file <- \"_skipped_for_manual_review.csv\"\n\nif (file.exists(skipped_file)) {\n  skipped_df <- read_csv(skipped_file, show_col_types = FALSE)\n\n  if (nrow(skipped_df) > 0) {\n    cat(\n      \"Starting interactive session to process\",\n      nrow(skipped_df),\n      \"skipped speeches.\\n\"\n    )\n\n    for (i in 1:nrow(skipped_df)) {\n      speech_info <- skipped_df[i, ]\n\n      #  Instructions for the user\n      cat(\"\\n--------------------------------------------------\\n\")\n      cat(\"--- PROCESSING SKIPPED SPEECH\", i, \"of\", nrow(skipped_df), \"---\\n\")\n      cat(\"          Date:\", speech_info$date, \"\\n\")\n      cat(\"    Parliament:\", speech_info$parliament, \"\\n\\n\")\n      cat(\"1. Open this URL in your browser:\\n\")\n      cat(\"  \", speech_info$skipped_url, \"\\n\\n\")\n      cat(\"2. Manually select and copy ONLY the text of the speech itself.\\n\\n\")\n\n      readline(\n        prompt = \"After copying the text, return here and press [Enter] to continue...\"\n      )\n\n      # Read the text vector directly from the system clipboard\n      speech_text_vector <- clipr::read_clip()\n\n      if (!is.null(speech_text_vector) && length(speech_text_vector) > 0) {\n        speech_text_single_string <- paste(speech_text_vector, collapse = \" \")\n\n        # Use the existing save_speech function with the single string\n        save_speech(\n          speech_text = speech_text_single_string,\n          date = speech_info$date,\n          parliament = speech_info$parliament\n        )\n      } else {\n        cat(\"\\nClipboard is empty. Skipping this speech.\\n\")\n      }\n    }\n    cat(\"\\nInteractive session complete.\\n\")\n  } else {\n    cat(\"The skipped speeches file is empty. Nothing to process.\\n\")\n  }\n} else {\n  cat(\"No '_skipped_for_manual_review.csv' file found. Nothing to do.\\n\")\n}\n```\n:::\n\n\n## Conclusion\nThe above process produces a folder with text files for each speech and an index file for future processing. \n\nIn the next step, we segment the speeches using Gemini LLM to classify sentences as part of: `opening_ceremonial`, `policy_content`, `closing_ceremonial` and `unclassified`. \n\n**Next Step:** [Segmentation of Throne Speeches using Gemini LLM →](02_Segmentation.qmd)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}