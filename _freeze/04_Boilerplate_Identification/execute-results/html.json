{
  "hash": "17965ee27c891e72847d609741d8e0f6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Boilerplate Identification\"\nauthor: \"Doron Fingold\"\ndate: \"today\"\nformat: \n  html:\n    toc: true\n    toc-location: right\n    self-contained: true\n    theme: cosmo\n    code-overflow: wrap\n    \n---\n\n## Overview\n\nThis document implements Phase 2 of the Throne Speech Cultural Analysis Workflow: **Refined Boilerplate Identification**. The primary goal is to analyze the ceremonial sections of the speeches to create a \"boilerplate dictionary\". This dictionary will contain recurring, formal phrases that are structural rather than substantive.\n\nThis process involves:  \n\n* Isolating the ceremonial text from the segmented corpus.  \n* Performing n-gram frequency analysis to find common phrases.   \n* Analyzing document frequency to find *pervasive* phrases that appear in most speeches.   \n* Compiling the results into a final dictionary.  \n\n----\n\n## Setup and Data Preparation\n\nFirst, we load the necessary libraries and the clean, segmented corpus. We'll then create a dedicated corpus containing only the ceremonial text for our analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1867)\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nsource(\"R/functions.R\")\n\n# Load the segmented corpus\nclean_corpus <- readr::read_csv(\n  \"output/segmentation/llm_clean_segmented_corpus.csv\"\n)\n\nceremonial_corpus <- clean_corpus %>%\n  select(filename, year, opening_ceremonial, closing_ceremonial) %>%\n  mutate(ceremonial_text = paste(opening_ceremonial, closing_ceremonial))\n\ntotal_docs <- nrow(ceremonial_corpus)\n```\n:::\n\n\n## Stopwords \nStopwords are commonly removed during text preprocessing because they appear frequently across documents but carry little semantic meaning that distinguishes one text from another - words like \"the,\" \"and,\" \"is,\" and \"of\" typically don't contribute to understanding the core topics or sentiment of a document. The `get_project_stopwords` function combines a standard set of stopwords (`tidytext::get_stopwords()`), with custom ones that appear with high frequency in throne speeches but don't help us identify topics. \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCustom Stopwords: \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"act\"        \"also\"       \"new\"        \"year\"       \"years\"     \n [6] \"measures\"   \"asked\"      \"national\"   \"make\"       \"continue\"  \n[11] \"provide\"    \"increase\"   \"great\"      \"ensure\"     \"need\"      \n[16] \"made\"       \"well\"       \"must\"       \"take\"       \"canada\"    \n[21] \"canada's\"   \"canadians\"  \"canadian\"   \"s\"          \"government\"\n[26] \"nation\"     \"dominion\"   \"bill\"       \"policy\"     \"minister\"  \n```\n\n\n:::\n:::\n\n\n## A Unified Analysis of Phrase Proportionality\n\nTo create a robust boilerplate dictionary, we will analyze n-grams from both ceremonial and policy texts simultaneously. We will identify phrases that are both highly pervasive across all speeches and disproportionately found in ceremonial sections. We do so using `analyze_ceremony_ngrams` function which takes in the size of the n-gram\" (number of words), in our case 2 and 3, the corpora and the stopwords regex, we prepared at the beginning, and returns a summary dataset comparing n-gram usage. \nThe table below, shows the top 12 n-grams.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Run the Analysis for N=2, 3\nall_ngrams_summary <- purrr::map_dfr(\n  c(2, 3),  # <- n-grams 2 and 3\n  analyze_ceremony_ngrams, # <- THIS IS WHERE THE FUNCTION IS CALLED\n  ceremonial_corpus, clean_corpus, stopword_regex # <- Other function arguments\n) %>%\n  select(\n    ngram,\n    n_size,\n    pervasiveness_pct,\n    ceremonial_proportion,\n    df_ceremonial,\n    df_policy\n  ) %>%\n  arrange(desc(pervasiveness_pct))\n\nknitr::kable(head(all_ngrams_summary, 12))\n```\n\n::: {.cell-output-display}\n\n\n|ngram              | n_size| pervasiveness_pct| ceremonial_proportion| df_ceremonial| df_policy|\n|:------------------|------:|-----------------:|---------------------:|-------------:|---------:|\n|governor general   |      2|          92.10526|             1.0000000|           140|        36|\n|black rod          |      2|          86.18421|             0.9847328|           129|         2|\n|speaker commanded  |      2|          86.18421|             0.9847328|           129|         2|\n|gentleman usher    |      2|          85.52632|             0.9846154|           128|         2|\n|gracious speech    |      2|          80.92105|             0.7642276|            94|        29|\n|senate chamber     |      2|          73.02632|             1.0000000|           111|         1|\n|divine providence  |      2|          61.84211|             0.9680851|            91|         4|\n|honourable members |      2|          57.23684|             0.9770115|            85|        20|\n|united states      |      2|          57.23684|             0.0919540|             8|        85|\n|senate members     |      2|          53.94737|             0.9634146|            79|        18|\n|follows honourable |      2|          46.71053|             0.7746479|            55|        16|\n|obedient servant   |      2|          42.76316|             1.0000000|            65|         0|\n\n\n:::\n:::\n\n\nVisualizing the results, this plot makes the trade-off clear. We are looking for phrases in the top-right quadrant.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](output/figures/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Creating the Dictionary \nWith these thresholds clearly defined, creating the dictionary is a simple, justifiable filtering step.\n\n- Lower `PERVASIVENESS_THRESHOLD` includes rarer phrases.\n- Lower `PROPORTIONALITY_THRESHOLD` includes phrases that are less purely ceremonial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# - Lower PERVASIVENESS_THRESHOLD to include rarer phrases.\n# - Lower PROPORTIONALITY_THRESHOLD to include phrases that are less purely ceremonial.\nPERVASIVENESS_THRESHOLD <- 10 # Lowered to 10% to catch more phrases\nPROPORTIONALITY_THRESHOLD <- 0.6 # At least 60% of its appearances must be ceremonial\n\n# Create the Dictionary\nintrusion_dictionary_df <- all_ngrams_summary %>%\n  filter(\n    pervasiveness_pct >= PERVASIVENESS_THRESHOLD,\n    ceremonial_proportion >= PROPORTIONALITY_THRESHOLD,\n    df_policy > 0\n  ) %>%\n  # Sort by n-gram size (longest first) to handle overlaps\n  arrange(desc(n_size))\n\n# Create the final regex from the sorted ngram column\nintrusion_regex <- paste(intrusion_dictionary_df$ngram, collapse = \"|\")\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nFinal Intrusion Dictionary created with 93 phrases.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|ngram                         | n_size| pervasiveness_pct| ceremonial_proportion| df_ceremonial| df_policy|\n|:-----------------------------|------:|-----------------:|---------------------:|-------------:|---------:|\n|follows honourable members    |      3|          40.78947|             0.8225806|            51|        11|\n|may divine providence         |      3|          36.84211|             0.9821429|            55|         1|\n|british north america         |      3|          28.28947|             0.6976744|            30|        15|\n|commons whose servant         |      3|          26.31579|             0.9500000|            38|         2|\n|duties thus assigned          |      3|          26.31579|             0.9750000|            39|         1|\n|important duties thus         |      3|          26.31579|             0.9750000|            39|         1|\n|knight grand cross            |      3|          25.65789|             0.9743590|            38|         1|\n|divine providence guide       |      3|          23.02632|             0.9714286|            34|         1|\n|country humbly claim          |      3|          22.36842|             0.9705882|            33|         1|\n|proceedings may receive       |      3|          22.36842|             0.9705882|            33|         1|\n|divine providence may         |      3|          16.44737|             0.9600000|            24|         1|\n|parliament honourable members |      3|          15.13158|             0.9565217|            22|         1|\n\n\n:::\n:::\n\n\n## Applying the Dictionary to Filter Policy Content\n\nFinally, we can demonstrate how to use this dictionary to filter these ceremonial phrases out of the policy_content section, as described in step 2.3 of the plan. This leaves a cleaner text for deeper thematic analysis later.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n--- ORIGINAL TEXT (Excerpt) ---\n Honourable Gentlemen of the Senate : Gentlemen of the House of Commons: It is\nwith much satisfaction that I again have recourse to your advice and assistance\nin the administration of the affairs of the Dominion. By the sudden and lamented\ndeath of the late Right Honourable Sir John Thompson, Cana... \n\n--- FILTERED TEXT (Excerpt) ---\n of the Senate : Gentlemen of the House of Commons: It is with much satisfaction\nthat I again have recourse to your advice and assistance in the administration\nof the affairs of the Dominion. By the sudden and lamented death of the late\nJohn Thompson, Canada has sustained a grievous loss. The de... \n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Breakdown of Average Word Counts After Cleaning Steps by Decade\n\n|Decade | Avg. Total Words| Stopwords Removed| Stopwords Removed (%)| Boilerplate Removed| Boilerplate Removed (%)| Substantive Words| Substantive Words (%)|\n|:------|----------------:|-----------------:|---------------------:|-------------------:|-----------------------:|-----------------:|---------------------:|\n|1860s  |            322.0|             176.5|                  54.8|                 1.0|                     0.3|             144.5|                  44.9|\n|1870s  |            808.1|             458.9|                  56.8|                 6.9|                     0.9|             342.3|                  42.4|\n|1880s  |            694.5|             389.8|                  56.1|                 2.5|                     0.4|             302.2|                  43.5|\n|1890s  |            578.4|             327.0|                  56.5|                 3.5|                     0.6|             247.8|                  42.8|\n|1900s  |            711.5|             389.5|                  54.7|                 5.8|                     0.8|             316.2|                  44.4|\n|1910s  |            641.5|             354.0|                  55.2|                 5.9|                     0.9|             281.6|                  43.9|\n|1920s  |            934.0|             499.4|                  53.5|                 5.1|                     0.5|             429.5|                  46.0|\n|1930s  |            874.9|             465.6|                  53.2|                 4.8|                     0.6|             404.5|                  46.2|\n|1940s  |            927.8|             489.3|                  52.7|                 7.8|                     0.8|             430.7|                  46.4|\n|1950s  |            953.1|             514.8|                  54.0|                 2.5|                     0.3|             435.9|                  45.7|\n|1960s  |           2176.2|            1168.6|                  53.7|                 5.0|                     0.2|            1002.5|                  46.1|\n|1970s  |           2398.8|            1229.8|                  51.3|                 3.8|                     0.2|            1165.2|                  48.6|\n|1980s  |           3170.7|            1583.5|                  49.9|                 5.7|                     0.2|            1581.5|                  49.9|\n|1990s  |           3584.6|            1852.6|                  51.7|                 6.2|                     0.2|            1725.8|                  48.1|\n|2000s  |           3576.0|            1808.4|                  50.6|                 3.4|                     0.1|            1764.2|                  49.3|\n|2010s  |           3612.0|            1793.2|                  49.6|                 4.0|                     0.1|            1814.8|                  50.2|\n|2020s  |           3460.3|            1697.3|                  49.1|                 6.3|                     0.2|            1756.7|                  50.8|\n\n\n:::\n:::\n\n\nWe observe that the *Substantive Words (%)* is about the same over the decades, ranging from 41.6 to 49.5. We can interpret this as an indication that our boilerplate + stopwords filter is general enough to have a similar impact. We might attribute the slight increase in Substantive words overtime as a function of the overall speech content increases over time.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](output/figures/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Save Intrusion Dictionary & Regex\n* Saving `intrusion_dictionary.csv`, dictionary phrases as a CSV for easy inspection.\n* Saving `intrusion_regex.rds`, regex string as an RDS file for efficient use in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutput_dir <- \"output/boilerplate/\"\nif (!dir.exists(output_dir)) {\n  dir.create(output_dir, recursive = TRUE)\n}\nreadr::write_csv(\n  intrusion_dictionary_df,\n  file.path(output_dir, \"intrusion_dictionary.csv\")\n)\nsaveRDS(\n  intrusion_regex,\n  file = file.path(output_dir, \"intrusion_regex.rds\")\n)\n```\n:::\n\n\n\n## Conclusion \n\nThe dictionary successfully removes the target phrases, demonstrating its utility for future analysis phases where we need to isolate purely policy-related text. \n\n**Next Step:** [LDA Topic Extraction and Analysis â†’](05_Topics_Extraction.qmd)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}