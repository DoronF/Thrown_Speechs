{
  "hash": "d12d6245073f8a55bbe15c5410d91d69",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Segmentation of Throne Speeches using Gemini LLM\"\nauthor: \"Doron Feingold\"\ndate: \"today\"\nformat: \n  html:\n    toc: true\n    toc-location: right\n    self-contained: true\n    theme: cosmo\n    code-overflow: wrap\n---\n\n## Overview\n\nIn the previous step, we scraped the Throne Speeches and saved them as text files. In this step we will segment the text. Manually dividing these speeches into their distinct rhetorical sections—the formal opening, the substantive policy agenda, and the ceremonial closing—is a time-consuming and subjective task. We will use Gemini LLM again to streamline the process.\n\nLLMs are very good but not perfect, so we will break the process into 2 stages: \n\n- **LLM Segmentation**: This stage automates the process by leveraging a Large Language Model (LLM), specifically Google's Gemini 1.5 Flash, via its API. The script loads a collection of speeches from text files, preprocesses them into a format the AI can understand, and then iteratively calls the Gemini API to identify the boundaries of each section. The final output is a set of structured CSV files, creating a clean, segmented corpus ready for analysis.\n\n- **Interactive Review**: The first stage identifies potential issue such as, \"Low policy content\", \"Excessive opening\", \"High unclassified content\", etc. The interactive review lets us review the flagged speeches and make corrections by re-classifying sentences as needed.\n\n***\n\n## Setup and Configuration\n\n\n### Loading Libraries\n\n::: {.cell}\n\n```{.r .cell-code}\n# Seed\nset.seed(1867)\n\n# Required Libraries\nlibrary(dplyr) # Data manipulation\nlibrary(tidytext) # Text preprocessing and tokenization\nlibrary(httr2) # For API requests\nlibrary(jsonlite) # For JSON handling\nlibrary(readr) # For reading/writing CSV\nlibrary(purrr) # For functional programming\nlibrary(stringr) # For string operations\nlibrary(crayon) # colouring Interactive Console\n```\n:::\n\n\nLoading functions and setting up environment variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"R/functions.R\")\n\nGEMINI_API_KEY <- Sys.getenv(\"GEMINI_API_KEY\")\n\nlog_file <- \"segmentation_log.txt\"\nlog_message(\"Starting throne speech segmentation script\", log_file)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[2025-09-07 15:36:24] Starting throne speech segmentation script \n```\n\n\n:::\n\n```{.r .cell-code}\noutput_dir <- \"output/segmentation/\"\nif (!dir.exists(output_dir)) {\n  dir.create(output_dir, recursive = TRUE)\n}\n```\n:::\n\n\n## Data Loading and Preprocessing\nThe analysis begins by loading the raw text of the speeches and transforming it into a structured format suitable for the LLM. \n\n**Note**: Some functions are in `R/functions.R`\n\n### Loading Speech Files\n`load_all_speeches` scans the throne_speeches_txt directory for all .txt files. For each file, it reads the content and extracts metadata (like Date: and Parliament:). It then compiles this into a single data frame, with one row per speech.\n\n### Tokenizing Speeches into Sentences\n`tokenize_speeches`: Once loaded, the raw text of each speech is tokenized into individual sentences using `tidytext::unnest_tokens`. Each sentence is then numbered, creating a \"numbered list\" format that we will provide to the AI for easy reference. This step produces two key data frames: one with every sentence as a row, and another that collapses the numbered sentences back into a single text block for each speech.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to Clean and Tokenize Speeches into Sentences\ntokenize_speeches <- function(speeches_df, log_file) {\n  log_message(\"Starting tokenization of speeches\", log_file)\n  speeches_sentences <- speeches_df %>%\n    mutate(\n      clean_text = str_replace_all(\n        speech_text,\n        \"[\\u201C\\u201D\\u2018\\u2019\\u201A\\u201E\\u2026\\u2013\\u2014]\",\n        \"'\"\n      ) %>%\n        str_replace_all(\"\\\\s+\", \" \") %>%\n        str_trim()\n    ) %>%\n    unnest_tokens(\n      sentence,\n      clean_text,\n      token = \"sentences\",\n      to_lower = FALSE\n    ) %>%\n    filter(str_length(sentence) > 10) %>%\n    group_by(filename) %>%\n    mutate(\n      sentence_id = row_number(),\n      sentence_position = sentence_id / max(sentence_id),\n      numbered_text = paste(sentence_id, sentence, sep = \": \")\n    ) %>%\n    ungroup()\n\n  speeches_numbered <- speeches_sentences %>%\n    group_by(filename, date, year, parliament, text_length) %>%\n    summarise(\n      full_numbered_text = paste(numbered_text, collapse = \"\\n\"),\n      total_sentences = n(),\n      .groups = \"drop\"\n    )\n\n  log_message(\n    paste(\n      \"Tokenized\",\n      nrow(speeches_numbered),\n      \"speeches into\",\n      sum(speeches_numbered$total_sentences),\n      \"sentences\"\n    ),\n    log_file\n  )\n  write_csv(\n    speeches_sentences,\n    \"output/segmentation/intermediate_sentences.csv\"\n  )\n  write_csv(speeches_numbered, \"output/segmentation/intermediate_numbered.csv\")\n  return(list(sentences = speeches_sentences, numbered = speeches_numbered))\n}\n```\n:::\n\n\n## Interacting with the Gemini API\n`process_speech_with_gemini` is the core of this stage, where we communicate with the Gemini LLM the following prompt:\n\n\n> Segment this full throne speech into three sections:  \n> 1. OPENING_CEREMONIAL: Formal protocol, addresses, historical acknowledgments, and introductory remarks (typically the first few paragraphs).  \n> 2. POLICY_CONTENT: Specific government agendas, commitments, and policy details (the bulk, often starting with phrases like \"Our Government will...\" or \"My Government will...\").  \n> 3. CLOSING_CEREMONIAL: Formal conclusion, blessings, or prorogation notes (typically the last paragraph).  \n> Provide the output as a JSON object with keys:  \n> - sections: An object with keys 'opening_ceremonial', 'policy_content', 'closing_ceremonial', each containing:  \n> - start_sentence_id: Integer (1-based)  \n> - end_sentence_id: Integer (1-based)  \n> - transition_markers: Array of strings (any identified transition phrases, e.g., \"Honourable Senators\", \"My Government will\", \"May Divine Providence\")\n>\n> Ensure the output is ONLY a single, valid JSON object with no extra text or formatting.\n\n\n\n### The Main Processing Loop\nThe `segment_speeches_llm` function orchestrates the entire segmentation process. It iterates through each speech, calling `process_speech_with_gemini` for each one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsegment_speeches_llm <- function(\n  speeches_numbered,\n  api_key,\n  max_retries = 3,\n  sleep_time = 2,\n  log_file\n) {\n  log_message(\"Starting LLM segmentation\", log_file)\n  # Load existing segmented results if available\n  segmented_list <- if (\n    file.exists(\"throne_speeches/intermediate_segmented_list.rds\")\n  ) {\n    readRDS(\"throne_speeches/intermediate_segmented_list.rds\")\n  } else {\n    list()\n  }\n  failed_speeches <- data.frame(filename = character(), error = character())\n\n  # Filter unprocessed speeches\n  speeches_to_process <- speeches_numbered %>%\n    filter(!filename %in% names(segmented_list))\n  log_message(\n    paste(\n      \"Processing\",\n      nrow(speeches_to_process),\n      \"unprocessed speeches\"\n    ),\n    log_file\n  )\n\n  for (i in 1:nrow(speeches_to_process)) {\n    row <- speeches_to_process[i, ]\n    attempt <- 1\n    result <- NULL\n\n    while (is.null(result) && attempt <= max_retries) {\n      log_message(paste(\"Attempt\", attempt, \"for\", row$filename), log_file)\n      result <- process_speech_with_gemini(\n        row$full_numbered_text,\n        row$filename,\n        api_key,\n        log_file = log_file\n      )\n      if (is.null(result)) {\n        log_message(\n          paste(\n            \"Retrying\",\n            row$filename,\n            \"after\",\n            sleep_time,\n            \"seconds\"\n          ),\n          log_file\n        )\n        Sys.sleep(sleep_time)\n        attempt <- attempt + 1\n      }\n    }\n\n    if (is.null(result)) {\n      failed_speeches <- rbind(\n        failed_speeches,\n        data.frame(filename = row$filename, error = \"Failed after max retries\")\n      )\n    }\n    segmented_list[[row$filename]] <- result\n\n    if (i %% 10 == 0) {\n      log_message(\n        paste(\n          \"...processed\",\n          i,\n          \"of\",\n          nrow(speeches_to_process),\n          \"speeches...\"\n        ),\n        log_file\n      )\n    }\n  }\n\n  log_message(\n    paste(\n      \"Completed LLM segmentation for\",\n      length(segmented_list),\n      \"speeches\"\n    ),\n    log_file\n  )\n  write_csv(failed_speeches, \"output/segmentation/failed_speeches.csv\")\n  saveRDS(segmented_list, \"output/segmentation/intermediate_segmented_list.rds\")\n  return(segmented_list)\n}\n```\n:::\n\n\n### Compiling Results and Generating Outputs\nAfter the API calls are complete, the script's final phase is to transform the raw JSON responses from the AI into the final, usable CSV files. `compile_segmented_data` takes the list of results returned by the API and joins it back to our sentence-by-sentence data frame. It uses the start_sentence_id and end_sentence_id provided by the AI for each section to label every sentence with its corresponding category (opening_ceremonial, policy_content, etc.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compile Segmented Data into Sentence-Level Dataframe\ncompile_segmented_data <- function(\n  speeches_sentences,\n  segmented_list,\n  log_file\n) {\n  log_message(\"Compiling segmented data\", log_file)\n  segmented_data <- speeches_sentences %>%\n    rowwise() %>%\n    mutate(\n      llm_result = list(segmented_list[[filename]]),\n      opening_start = llm_result$sections$opening_ceremonial$start_sentence_id %||%\n        NA,\n      opening_end = llm_result$sections$opening_ceremonial$end_sentence_id %||%\n        NA,\n      policy_start = llm_result$sections$policy_content$start_sentence_id %||%\n        NA,\n      policy_end = llm_result$sections$policy_content$end_sentence_id %||% NA,\n      closing_start = llm_result$sections$closing_ceremonial$start_sentence_id %||%\n        NA,\n      closing_end = llm_result$sections$closing_ceremonial$end_sentence_id %||%\n        NA,\n      section = case_when(\n        is.na(opening_start) | is.na(policy_start) | is.na(closing_start) ~\n          \"unclassified\",\n        between(sentence_id, opening_start, opening_end) ~ \"opening_ceremonial\",\n        between(sentence_id, policy_start, policy_end) ~ \"policy_content\",\n        between(sentence_id, closing_start, closing_end) ~ \"closing_ceremonial\",\n        TRUE ~ \"unclassified\"\n      )\n    ) %>%\n    ungroup()\n\n  log_message(\"Completed compilation of segmented data\", log_file)\n  return(segmented_data)\n}\n```\n:::\n\n\n### Generating Final Output Files\nThe `generate_segmented_outputs` function creates the final deliverables. It aggregates the sentence-level data to produce several summary tables: a clean corpus with one row per speech, a high-level summary of section proportions, and a list of speeches flagged for potential review.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_segmented_outputs <- function(segmented_data, log_file) {\n  log_message(\n    \"Generating outputs: summaries, corpus, validation, and issues\",\n    log_file\n  )\n\n  # Speech Summaries\n  speech_summaries <- segmented_data %>%\n    group_by(filename, date, year, parliament) %>%\n    summarise(\n      total_sentences = n(),\n      opening_sentences = sum(section == \"opening_ceremonial\"),\n      policy_sentences = sum(section == \"policy_content\"),\n      closing_sentences = sum(section == \"closing_ceremonial\"),\n      unclassified_sentences = sum(section == \"unclassified\"),\n      .groups = \"drop\"\n    ) %>%\n    mutate(\n      opening_pct = round(100 * opening_sentences / total_sentences, 1),\n      policy_pct = round(100 * policy_sentences / total_sentences, 1),\n      closing_pct = round(100 * closing_sentences / total_sentences, 1),\n      unclassified_pct = round(\n        100 * unclassified_sentences / total_sentences,\n        1\n      ),\n      era = case_when(\n        year <= 1920 ~ \"Early Confederation\",\n        year <= 1960 ~ \"Mid-20th Century\",\n        year <= 1990 ~ \"Late 20th Century\",\n        TRUE ~ \"Contemporary\"\n      )\n    )\n\n  # Clean Corpus\n  clean_corpus <- segmented_data %>%\n    group_by(filename, date, year, parliament) %>%\n    summarise(\n      opening_ceremonial = paste(\n        sentence[section == \"opening_ceremonial\"],\n        collapse = \" \"\n      ),\n      policy_content = paste(\n        sentence[section == \"policy_content\"],\n        collapse = \" \"\n      ),\n      closing_ceremonial = paste(\n        sentence[section == \"closing_ceremonial\"],\n        collapse = \" \"\n      ),\n      opening_sentence_count = sum(section == \"opening_ceremonial\"),\n      policy_sentence_count = sum(section == \"policy_content\"),\n      closing_sentence_count = sum(section == \"closing_ceremonial\"),\n      opening_word_count = str_count(opening_ceremonial, \"\\\\S+\") %||% 0,\n      policy_word_count = str_count(policy_content, \"\\\\S+\") %||% 0,\n      closing_word_count = str_count(closing_ceremonial, \"\\\\S+\") %||% 0,\n      total_sentences = n(),\n      .groups = \"drop\"\n    ) %>%\n    mutate(\n      era = case_when(\n        year <= 1920 ~ \"Early Confederation\",\n        year <= 1960 ~ \"Mid-20th Century\",\n        year <= 1990 ~ \"Late 20th Century\",\n        TRUE ~ \"Contemporary\"\n      ),\n      decade = paste0(floor(year / 10) * 10, \"s\"),\n      policy_content_clean = str_replace_all(\n        policy_content,\n        \"[\\u201C\\u201D\\u2018\\u2019\\u201A\\u201E\\u2026\\u2013\\u2014]\",\n        \"'\"\n      ) %>%\n        str_replace_all(\"\\\\s+\", \" \") %>%\n        str_trim(),\n      unclassified_sentences = total_sentences -\n        (opening_sentence_count +\n          policy_sentence_count +\n          closing_sentence_count),\n      needs_review = policy_sentence_count < 5 |\n        policy_word_count < 100 |\n        unclassified_sentences > 0\n    )\n\n  # Validation by Era (This will now work correctly)\n  validation_stats <- speech_summaries %>%\n    group_by(era) %>%\n    summarise(\n      n_speeches = n(),\n      avg_opening_pct = round(mean(opening_pct, na.rm = TRUE), 1),\n      avg_policy_pct = round(mean(policy_pct, na.rm = TRUE), 1),\n      avg_closing_pct = round(mean(closing_pct, na.rm = TRUE), 1),\n      avg_unclassified_pct = round(mean(unclassified_pct, na.rm = TRUE), 1),\n      .groups = \"drop\"\n    )\n\n  # Flag Issues\n  issues <- speech_summaries %>%\n    mutate(\n      issue = case_when(\n        policy_pct < 40 ~ \"Low policy content\",\n        opening_pct > 40 ~ \"Excessive opening\",\n        closing_pct > 40 ~ \"Excessive closing\",\n        policy_sentences < 10 ~ \"Very few policy sentences\",\n        unclassified_pct > 10 ~ \"High unclassified content\",\n        TRUE ~ \"OK\"\n      )\n    ) %>%\n    filter(issue != \"OK\")\n\n  log_message(\n    paste(\n      \"Generated outputs:\",\n      nrow(speech_summaries),\n      \"summaries,\",\n      nrow(clean_corpus),\n      \"corpus entries,\",\n      nrow(issues),\n      \"issues flagged\"\n    ),\n    log_file\n  )\n  return(list(\n    summaries = speech_summaries,\n    corpus = clean_corpus,\n    validation = validation_stats,\n    issues = issues\n  ))\n}\n```\n:::\n\n\n### Execution\nThis is the main execution block that runs the entire process from start to finish. The functions defined in the previous steps are called in logical order.\n\n**Note**: This code chunk is set to `eval=FALSE` by default to prevent it from automatically running when you render the document.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Main Execution\nmain <- function(base_dir = \"output/throne_speeches_txt\", api_key) {\n  log_message(\"=== Starting Main Execution ===\", log_file)\n\n  # Step 1: Load speeches\n  speeches_df <- load_all_speeches(base_dir, log_file)\n\n  # Step 2: Tokenize and prepare numbered text\n  tokenized <- tokenize_speeches(speeches_df, log_file)\n  speeches_sentences <- tokenized$sentences\n  speeches_numbered <- tokenized$numbered\n\n  # Step 3: Process with Gemini API\n  segmented_list <- segment_speeches_llm(\n    speeches_numbered,\n    api_key,\n    log_file = log_file\n  )\n\n  # Step 4: Compile segmented data\n  segmented_data <- compile_segmented_data(\n    speeches_sentences,\n    segmented_list,\n    log_file\n  )\n\n  # Step 5: Generate outputs\n  outputs <- generate_segmented_outputs(segmented_data, log_file)\n\n  # Save results\n  log_message(\"Saving output files\", log_file)\n  write_csv(segmented_data, \"output/segmentation/llm_detailed_segmentation.csv\")\n  write_csv(\n    outputs$corpus,\n    \"output/segmentation/llm_clean_segmented_corpus.csv\"\n  )\n  write_csv(outputs$summaries, \"output/segmentation/llm_speech_summaries.csv\")\n  write_csv(outputs$validation, \"output/segmentation/llm_validation_stats.csv\")\n  write_csv(outputs$issues, \"output/segmentation/llm_issues.csv\")\n\n  # Print summaries\n  cat(\"\\n=== FINAL CORPUS SUMMARY ===\\n\")\n  print(\n    outputs$summaries %>%\n      select(\n        filename,\n        year,\n        opening_pct,\n        policy_pct,\n        closing_pct,\n        unclassified_pct\n      ) %>%\n      head()\n  )\n\n  cat(\"\\n=== VALIDATION BY ERA ===\\n\")\n  print(outputs$validation)\n\n  cat(\"\\n=== SPEECHES FLAGGED FOR REVIEW ===\\n\")\n  cat(\n    \"Number flagged:\",\n    nrow(outputs$issues),\n    \"out of\",\n    nrow(outputs$summaries),\n    \"speeches\\n\"\n  )\n  if (nrow(outputs$issues) > 0) {\n    print(outputs$issues %>% select(filename, year, issue))\n  }\n\n  log_message(\"=== Script Execution Completed ===\", log_file)\n  return(list(\n    segmented_data = segmented_data,\n    outputs = outputs\n  ))\n}\n\nresults <- main(\n  base_dir = \"output/throne_speeches_txt\",\n  api_key = GEMINI_API_KEY\n)\n```\n:::\n\n\n***\n\n## Interactive Review of Identified Issues\nAn interactive tool to review and correct LLM-based speech segmentations. The script loops through the speeches identified and recorded in llm_issues.csv. The screenshot below illustrates the options available.\n\n\n![Screenshot of the main menu for the interactive console review script below](figures/Segmentation_Prompt.png)\n\n**Note**: The script below will only run on R console (most IDEs should be fine. I used Positron with no issues).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"R/interactive_segmentation_review.R\")\n```\n:::\n\n\n## Conclusion \n\nIn this step, we segmented the sentences using a two-stage approach where we ask an LLM to segment our text. We used logical thresholds to identify issues for review. Reviewed and made corrections. \n \n\n**Next Step:** [Exploratory Analysis of Throne Speeches →](03_Exploratory_Analysis.qmd)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}